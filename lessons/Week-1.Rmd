---
title: 'Week 1: Data Import, Manipulation, and GitHub'
output:
  html_document:
    df_print: paged
  github_document: default
---

<!--
html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
-->

<!-- output: rmarkdown::github_document -->

# Objectives:

- Set up and configure GitHub for use within RStudio.  
- Importing data in R (a quick overview).  
- Data manipulation in R (a quick overview).

# Setup GitHub

Follow instructions on [Using Git with RStudio](https://jennybc.github.io/2014-05-12-ubc/ubc-r/session03_git.html). 

For a Windows machine, generally, it is:

- Download and install [Git](https://git-scm.com/download/win).  
- In RStudio, create an SSH key pair (follow instructions [here](https://happygitwithr.com/ssh-keys.html#create-an-ssh-key-pair)):  
  - Go to Tools > Global Options... > Git/SVN.  
  - Point to the executable "git.exe" (/Program Files/Git/)  
  - Create RSA key in RStudio.  
  - Copy RSA key. Add the generated key by clicking "new SSH key" into GitHub under your profile's settings > SSH and GPG keys, and pasting it.
- Open git bash executable, and change directory (`cd`) to the folder you want to save the repo in.  
- Run the following:  
```
git clone https://github.com/engineerchange/r-dsandml-exercises.git
```

- Open .Rproj file in RStudio.  
- Then go to Tools > Project Options... > Git/Svn to configure.  
- From there, you should see a "Git" tab in the Environment pane where you can Commit, Pull, and Push.  

# Importing 
<!-- read_csv, read_csv2, read_delim, data.table, dtplyr, arrow/parquet -->

Load in necessary libraries.

Data is sourced from Euro Spatial Diffusion Observatory (ESDO) in-person surveys between 2002 and 2011 in France. From [Mendeley](https://data.mendeley.com/datasets/f257j67ym6/2).

```{r, message=FALSE}
library(tidyverse)
library(gghighlight)
library(gt)
library(arrow)
library(microbenchmark) # to do simple benchmarking
dir = here::here()
df = read_csv2(paste0(dir,"/lessons/data/DB_ESDO_FRANCE_2002-2011.csv")) # initial un-cached dataset
```

Tidyverse's read_csv() function, which is 10x faster than base's read.csv() function.

Compare both:

```{r, message=FALSE, cache=TRUE}

# note we use read.csv2 and read_csv2 because these files are delineated with semi-colons.

func1 <- function(){read.csv2(paste0(dir,"/lessons/data/DB_ESDO_FRANCE_2002-2011.csv"))}
func2 <- function(){read_csv2(paste0(dir,"/lessons/data/DB_ESDO_FRANCE_2002-2011.csv"))}

out = microbenchmark(times=20, unit="ms", func1(), func2())
print(out)
```

Can potentially speed up operations after by saving as RDS or arrow's parquet.

```{r, message=FALSE, cache=TRUE}

df = read_csv2(paste0(dir,"/lessons/data/DB_ESDO_FRANCE_2002-2011.csv"))

func1 <- function(){write_rds(df,paste0(dir,"/lessons/data/out.RDS"))}
func2 <- function(){write_parquet(df,paste0(dir,"/lessons/data/out.parquet"))}

out = microbenchmark(times=20, unit="ms", func1(), func2())
print(out)
```

Can potentially speed up read times with parquet over standard RDS.

```{r, message=FALSE, cache=TRUE}

func1 <- function(){read_rds(paste0(dir,"/lessons/data/out.RDS"))}
func2 <- function(){read_parquet(paste0(dir,"/lessons/data/out.parquet"))}

out = microbenchmark(times=20, unit="ms", func1(), func2())
print(out)
```

Try just reading in specific columns starting with QFRA.

```{r, message=FALSE, cache=TRUE}

func1 <- function(){read_rds(paste0(dir,"/lessons/data/out.RDS")) %>% select(starts_with("QFRA"))}
func2 <- function(){read_parquet(paste0(dir,"/lessons/data/out.parquet"),col_select = starts_with("QFRA"))}

out = microbenchmark(times=20, unit="ms", func1(), func2())
print(out)

```

# Data Manipulation

Get an idea of what data there is.

```{r, message=FALSE, cache=TRUE}
head(df,5)

# read metadata

df2 = read_csv2(paste0(dir,"/lessons/data/metadataDB_ESDO_FRANCE_2002-2011.csv"))
head(df2,20)
head(df2 %>% slice(20:nrow(df2)),20)
```

Apparently, there are a lot of columns of extra data, so we exclude to just the France coin related columns and demographics we want.

```{r}
df = df %>% select(SURVEY:AGE,starts_with("QFRA"))
head(df)
```

We may want to get an idea of the average proportion of different coin types by individual. So we can graph that.

```{r,message=FALSE}

# Make data in long form

df_long = df %>%
  mutate(Person=row_number()) %>%
  pivot_longer(cols=starts_with("QFRA"),names_to = "Coin", values_to = "Count")
head(df_long,5)

# Separate out column Coin

df_long = df_long %>%
  separate(Coin,sep="_",into = c("Lbl","Coin"))
head(df_long,5)

# remove column Lbl

df_long = df_long %>% select(-Lbl)
head(df_long,5)

# rearrange columns

df_long = df_long %>% select(Person,SURVEY,SEX,AGE,everything())
head(df_long,5)

# make initial bar chart

df_long %>% group_by(Coin,Count) %>% count() %>% ungroup() %>%
  ggplot() + theme_bw() +
  geom_bar(aes(x=Coin,y=n,fill=as.character(Count)),stat='identity',position='stack')

# make bar chart ignoring 0 counts

df_long %>% dplyr::filter(Count!="0") %>%
  group_by(Coin,Count) %>% count() %>% ungroup() %>%
  ggplot() + theme_bw() +
  geom_bar(aes(x=Coin,y=n,fill=as.character(Count)),stat='identity',position='stack')

# group coins into buckets

df_long %>% dplyr::filter(Count!="0") %>%
  mutate(Count=case_when(
    Count %in% c(1,2,3) ~ "1-3",
    Count %in% c(4,5,6) ~ "4-6",
    Count %in% c(7,8,9) ~ "7-9",
    TRUE ~ "10+"
  )) %>%
  group_by(Coin,Count) %>% count() %>% ungroup() %>%
  ggplot() + theme_bw() +
  geom_bar(aes(x=Coin,y=n,fill=Count),stat='identity',position='stack')
  
# clean up: add axis labels, title, scale axis, viridis discrete scale

df_long %>% dplyr::filter(Count!="0") %>%
  mutate(Count=case_when(
    Count %in% c(1,2,3) ~ "1-3",
    Count %in% c(4,5,6) ~ "4-6",
    Count %in% c(7,8,9) ~ "7-9",
    TRUE ~ "10+"
  )) %>% 
  group_by(Coin,Count) %>% count() %>% ungroup() %>%
  ggplot() + theme_bw() +
  geom_bar(aes(x=factor(Coin, level = c("1c","1e","2c","2e","5c","10c","20c","50c")),y=n,fill=Count),
           stat='identity',position='stack') +
  scale_x_discrete(name="French coins") +
  scale_y_continuous(name="Count of coins",labels = scales::comma) +
  ggtitle("Count of French coins per person") +
  scale_fill_viridis_d(name="Count per person")


```

<!-- pivot_wider, pivot_longer, separate, unite -->

# Exploratory Data Analysis (EDA)

Exploratory Data Analysis is a fancy way of saying explore the data.

Why?  
- Understand what data there is (features, records, relationships, etc.)  
- Under what data there isn't (features missing, data missing)  
- Data problems (misspellings, variable usage / data collection over time, missing data)  
- Distribution of data (is it normally distributed? is it a sample?)

TL;DR - we ask questions of the data before doing any modelling.  

Let's look at dates of these surveys.  

```{r,message=FALSE}
df %>% mutate(SURVEY=as.Date(SURVEY,"%m/%d/%Y") %>% as.character()) %>%
  group_by(SURVEY) %>% count() %>% ungroup() %>%
  ggplot() +
  geom_bar(aes(x=SURVEY,y=n),stat='identity') +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  scale_x_discrete(name="Date of Survey") + scale_y_continuous(name="Count per survey") +
  ggtitle("Surveyed per survey date")

# do with date
df %>% mutate(SURVEY=as.Date(SURVEY,"%m/%d/%Y")) %>%
  group_by(SURVEY) %>% count() %>% ungroup() %>%
  ggplot() +
  geom_bar(aes(x=SURVEY,y=n,group=SURVEY),stat='identity',width = 10,colour="red") +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  scale_x_date(name="Date of Survey") + scale_y_continuous(name="Count per survey") +
  ggtitle("Surveyed per survey date")

# do our best to dodge the bars
df %>% mutate(SURVEY=as.Date(SURVEY,"%m/%d/%Y")) %>%
  group_by(SURVEY) %>% count() %>% ungroup() %>%
  ggplot() +
  geom_bar(aes(x=SURVEY,y=n,group=SURVEY),stat='identity',width = 100,colour="red",position="dodge2") +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  scale_x_date(name="Date of Survey") + scale_y_continuous(name="Count per survey") +
  ggtitle("Surveyed per survey date")
```

Let's look at age.  

```{r}

# try to graph age by survey
df %>% mutate(SURVEY=as.Date(SURVEY,"%m/%d/%Y")) %>%
    group_by(SURVEY,AGE) %>% count() %>% ungroup() %>%
  ggplot() + geom_point(aes(x=AGE,y=SURVEY,size=n)) +
  theme_bw()

# let's jitter the points
df %>% mutate(SURVEY=as.Date(SURVEY,"%m/%d/%Y")) %>%
    group_by(SURVEY,AGE) %>% count() %>% ungroup() %>%
  ggplot() + geom_jitter(aes(x=AGE,y=SURVEY,size=n),width=50) +
  theme_bw()

# boxplot
df %>% mutate(SURVEY=as.Date(SURVEY,"%m/%d/%Y") %>% as.character()) %>%
    group_by(SURVEY,AGE) %>% count() %>% ungroup() %>%
  ggplot() + geom_boxplot(aes(x=SURVEY,y=AGE,fill=SURVEY)) +
  scale_fill_discrete(guide="none") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=90, hjust=1))

# boxplot + points
df %>% mutate(SURVEY=as.Date(SURVEY,"%m/%d/%Y") %>% as.character()) %>%
    group_by(SURVEY,AGE) %>% count() %>% ungroup() %>%
  ggplot(aes(x=SURVEY,y=AGE,fill=SURVEY)) + 
  geom_boxplot() +
  geom_jitter() +
  scale_fill_discrete(guide="none") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=90, hjust=1))

# distribution of age
df %>% group_by(AGE) %>% count() %>% ungroup() %>%
  ggplot() + 
  geom_bar(aes(x=AGE,y=n),stat='identity') +
  scale_x_continuous(breaks=c(seq(0,100,5))) +
  theme_bw()

# gghighlight
df %>% group_by(AGE) %>% count() %>% ungroup() %>%
  ggplot() + 
  geom_bar(aes(x=AGE,y=n),stat='identity') +
  scale_x_continuous(breaks=c(seq(0,100,5))) +
  gghighlight(AGE==40) + 
  geom_label(aes(x=AGE,y=n,label=AGE)) +
  theme_bw() +
  ggtitle("Distribution of age across all surveys")
```

```{r, message=FALSE}
df %>%
  mutate(AGE_GROUP = case_when(
    AGE < 20 ~ "<20",
    AGE >= 20 & AGE <30 ~ "20s",
    AGE >= 30 & AGE <40 ~ "30s",
    AGE >= 40 & AGE <50 ~ "40s",
    AGE >= 50 & AGE <60 ~ "50s",
    AGE >= 60 & AGE <70 ~ "60s",
    AGE >= 70 & AGE <80 ~ "70s",
    AGE >= 80 & AGE <90 ~ "80s",
    AGE >= 90 & AGE <100 ~ "90s",
    AGE >= 100 ~ "100+",
    TRUE ~ "ELSE"
  )) %>%
  group_by(AGE_GROUP) %>% count() %>% ungroup() %>%
  gt()
```

<!-- geom_point, geom_histogram, geom_boxplot, gt, left_join, anti_join -->


# Resources

## GitHub
- [Using Git with RStudio](https://jennybc.github.io/2014-05-12-ubc/ubc-r/session03_git.html), Jenny Bryan  
- [Happy Git with R](https://happygitwithr.com/), Jenny Bryan  

## Importing
- [R for Data Science - Data Import](https://r4ds.had.co.nz/data-import.html)  
- [Efficient input/output](https://csgillespie.github.io/efficientR/input-output.html)  
- [Apache Arrow](https://arrow.apache.org/docs/r/reference/index.html)